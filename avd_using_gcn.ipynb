{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Anomaly detection using Graph Convolutional Network\n",
    "====================================\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Overview\n",
    "------------------------------------------\n",
    "GCN from the perspective of message passing\n",
    "```````````````````````````````````````````````\n",
    "We describe a layer of graph convolutional neural network from a message\n",
    "passing perspective; the math can be found `here <math_>`_.\n",
    "It boils down to the following step, for each node $u$:\n",
    "\n",
    "1) Aggregate neighbors' representations $h_{v}$ to produce an\n",
    "intermediate representation $\\hat{h}_u$.  2) Transform the aggregated\n",
    "representation $\\hat{h}_{u}$ with a linear projection followed by a\n",
    "non-linearity: $h_{u} = f(W_{u} \\hat{h}_u)$.\n",
    "\n",
    "We will implement step 1 with DGL message passing, and step 2 with the\n",
    "``apply_nodes`` method, whose node UDF will be a PyTorch ``nn.Module``.\n",
    "\n",
    "GCN implementation with DGL\n",
    "``````````````````````````````````````````\n",
    "We first define the message and reduce function as usual.  Since the\n",
    "aggregation on a node $u$ only involves summing over the neighbors'\n",
    "representations $h_v$, we can simply use builtin functions:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import dgl\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "NODES = 1000\n",
    "EDGES = 20000\n",
    "\n",
    "\n",
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then define the node UDF for ``apply_nodes``, which is a fully-connected layer:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then proceed to define the GCN module. A GCN layer essentially performs\n",
    "message passing on all the nodes then applies the `NodeApplyModule`. Note\n",
    "that we omitted the dropout in the paper for simplicity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(gcn_msg, gcn_reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The forward function is essentially the same as any other commonly seen NNs\n",
    "model in PyTorch.  We can initialize GCN like any ``nn.Module``. For example,\n",
    "let's define a simple neural network consisting of two GCN layers. Suppose we\n",
    "are training the classifier for the cora dataset (the input feature size is\n",
    "1433 and the number of classes is 7).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (gcn1): GCN(\n",
      "    (apply_mod): NodeApplyModule(\n",
      "      (linear): Linear(in_features=1000, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (gcn2): GCN(\n",
      "    (apply_mod): NodeApplyModule(\n",
      "      (linear): Linear(in_features=16, out_features=2, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(NODES, 16, F.relu)\n",
    "        self.gcn2 = GCN(16, 2, F.relu)\n",
    "    \n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        x = self.gcn2(g, x)\n",
    "        return x\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the cora dataset using DGL's built-in data module.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = th.FloatTensor(data.features)\n",
    "    labels = th.LongTensor(data.labels)\n",
    "    print(data.labels)\n",
    "    mask = th.ByteTensor(data.train_mask)\n",
    "    g = data.graph\n",
    "    # add self loop\n",
    "    g.remove_edges_from(g.selfloop_edges())\n",
    "    g = DGLGraph(g)\n",
    "    g.add_edges(g.nodes(), g.nodes())\n",
    "    return g, features, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_my_data():\n",
    "    g = dgl.DGLGraph()\n",
    "    \n",
    "    g.add_nodes(NODES)\n",
    "    \n",
    "    edge_list = []\n",
    "    \n",
    "    # create at least one edge from every node\n",
    "    for node in range(g.number_of_nodes()):\n",
    "        random_node = random.randint(0, g.number_of_nodes()-1)\n",
    "        while random_node == node:\n",
    "            random_node = random.randint(0, g.number_of_nodes()-1)\n",
    "        edge_list.append((node, random_node))\n",
    "    \n",
    "    #print(\"Edge list:\", edge_list)\n",
    "    \n",
    "    # create additional edges\n",
    "    for _ in range(EDGES):\n",
    "        random_node1 = random.randint(0, g.number_of_nodes()-1)\n",
    "        random_node2 = random.randint(0, g.number_of_nodes()-1)\n",
    "        \n",
    "        while random_node1 == random_node2:\n",
    "            random_node1 = random.randint(0, g.number_of_nodes()-1)\n",
    "        edge = (random_node1, random_node2)\n",
    "        r_edge = (random_node2, random_node1)\n",
    "        \n",
    "        if edge in edge_list or r_edge in edge_list:\n",
    "            continue\n",
    "        else:\n",
    "            edge_list.append(edge)    \n",
    "    \n",
    "    #print(\"Edge list:\", edge_list)\n",
    "    # add edges two lists of nodes: src and dst\n",
    "    src, dst = tuple(zip(*edge_list))\n",
    "    g.add_edges(src, dst)\n",
    "    # edges are directional in DGL; make them bi-directional\n",
    "    g.add_edges(dst, src)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels...\n",
      "Loading graph...\n",
      "Data loaded.\n",
      "2320975\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from avd.configs import config\n",
    "from avd.datasets.twitter import load_data\n",
    "\n",
    "def load_twitter(twitter_graph, twitter_config):\n",
    "    g = dgl.DGLGraph()\n",
    "    \n",
    "    labels = {\"neg\": \"Real\", \"pos\": \"Fake\"}\n",
    "    twitter_graph, twitter_config = load_data(dataset_file_name=\"twitter_filtered.csv\", \n",
    "                                          labels_file_name=\"twitter_labels_filtered.csv\", \n",
    "                                          labels_map=labels, \n",
    "                                          limit=5000000) # Loads filtered dataset.\n",
    "    print(len(twitter_graph.vertices))\n",
    "    \n",
    "    \"\"\"\n",
    "    # look into graph_factory.py\n",
    "    # Add nodes\n",
    "    g.add_nodes(NODES)\n",
    "    \n",
    "    # Add edges\n",
    "    edge_list = []\n",
    "    for every line in edge file:\n",
    "        src, dst = tuple(zip(*edge_list))\n",
    "        g.add_edges(src, dst)\n",
    "        \n",
    "    # Add labels\n",
    "    # Look in abstract_graph.py/write_nodes_labels()\n",
    "    \"\"\"\n",
    "    #g.from_networkx(twitter_graph)#, edge_attrs=['e1', 'e2'])\n",
    "\n",
    "    #return g, features, labels, mask\n",
    "    return g\n",
    "\n",
    "g = load_twitter(twitter_graph, twitter_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then train the network as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = load_my_data()\n",
    "\n",
    "features = th.eye(NODES)\n",
    "labels_list = [0] * NODES\n",
    "for i in range(NODES//4):\n",
    "    node = random.randint(0, NODES-1)\n",
    "    labels_list[node] = 1\n",
    "labels = th.LongTensor(labels_list)\n",
    "\n",
    "mask_list = [0] * NODES\n",
    "for i in range(NODES//5):\n",
    "    labels_list[i] = 1\n",
    "\n",
    "mask = th.BoolTensor(mask_list)\n",
    "\n",
    "print(\"Number of nodes:\", g.number_of_nodes())\n",
    "print(\"Number of edges:\", g.number_of_edges())\n",
    "print(\"Features:\", type(features), features.shape)\n",
    "print(\"Labels:\", type(labels), labels.shape)\n",
    "print(\"mask:\", type(mask), mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "#g, features, labels, mask = load_cora_data()\n",
    "#print(\"Features:\", type(features), features.shape)\n",
    "#print(\"Labels:\", type(labels), labels.shape)\n",
    "#print(\"mask:\", type(mask), mask.shape)\n",
    "\n",
    "def acc(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-3)\n",
    "dur = []\n",
    "for epoch in range(200):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "        \n",
    "    logits = net(g, features)\n",
    "    #print(\"logits:\", type(logits), logits.shape)\n",
    "    #print(\"logits:\", logits)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    #print(\"logp:\", type(logp), logp.shape)\n",
    "    #print(\"logp[mask]:\", logp[mask].shape)\n",
    "    #print(\"mask.shape:\", mask.shape, \"logp.shape:\", logp.shape, \"logp[mask].shape:\", logp[mask].shape, \"labels.shape:\",labels[mask].shape)\n",
    "    \n",
    "    loss = F.nll_loss(logp, labels)\n",
    "    accuracy = acc(logits, labels)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "    \n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Accuracy {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch+1, loss.item(), accuracy, np.mean(dur)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "GCN in one formula\n",
    "------------------\n",
    "Mathematically, the GCN model follows this formula:\n",
    "\n",
    "$H^{(l+1)} = \\sigma(\\tilde{D}^{-\\frac{1}{2}}\\tilde{A}\\tilde{D}^{-\\frac{1}{2}}H^{(l)}W^{(l)})$\n",
    "\n",
    "Here, $H^{(l)}$ denotes the $l^{th}$ layer in the network,\n",
    "$\\sigma$ is the non-linearity, and $W$ is the weight matrix for\n",
    "this layer. $D$ and $A$, as commonly seen, represent degree\n",
    "matrix and adjacency matrix, respectively. The ~ is a renormalization trick\n",
    "in which we add a self-connection to each node of the graph, and build the\n",
    "corresponding degree and adjacency matrix.  The shape of the input\n",
    "$H^{(0)}$ is $N \\times D$, where $N$ is the number of nodes\n",
    "and $D$ is the number of input features. We can chain up multiple\n",
    "layers as such to produce a node-level representation output with shape\n",
    ":math`N \\times F`, where $F$ is the dimension of the output node\n",
    "feature vector.\n",
    "\n",
    "The equation can be efficiently implemented using sparse matrix\n",
    "multiplication kernels (such as Kipf's\n",
    "`pygcn <https://github.com/tkipf/pygcn>`_ code). The above DGL implementation\n",
    "in fact has already used this trick due to the use of builtin functions. To\n",
    "understand what is under the hood, please read our tutorial on :doc:`PageRank <../../basics/3_pagerank>`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
