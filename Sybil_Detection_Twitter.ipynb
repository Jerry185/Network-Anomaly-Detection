{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sybil Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from avd.graph_learning_controller import GraphLearningController\n",
    "from avd.learners.sklearner import SkLearner\n",
    "from avd.configs import config\n",
    "from avd.datasets.twitter import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify output directory for classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_folder = os.getcwd() + \"/data/output\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define what is considered positive and negative labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\"neg\": \"Real\", \"pos\": \"Fake\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the Twitter graph.\n",
    "load_data will return a graph object(twitter_graph) and a config object(twitter_config)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading labels...\n",
      "Loading graph...\n",
      "Data loaded.\n",
      "75624\n"
     ]
    }
   ],
   "source": [
    "twitter_graph, twitter_config = load_data(dataset_file_name=\"twitter_filtered.csv\", labels_file_name=\"twitter_labels_filtered.csv\", \n",
    "                                          labels_map=labels, limit=5000000) # Loads filtered dataset.\n",
    "print(len(twitter_graph.vertices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the extracted feature can be useful for understanding the result, but they will not be used in the classification proccess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if twitter_graph.is_directed:\n",
    "    meta_data_cols = [\"dst\", \"src\", \"out_degree_v\", \"in_degree_v\", \"out_degree_u\", \"in_degree_u\"]\n",
    "else:\n",
    "    meta_data_cols = [\"dst\", \"src\", \"number_of_friends_u\", \"number_of_friends_v\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we train the classification algorithm.\n",
    "The Twitter dataset is incomplete and missing a lot of data.\n",
    "In order to deal with this problem, we train 10 times and aggregate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting training and test sets\n",
      "Existing files were loaded.\n",
      "Training 10-fold validation: {'accuracy': 0.8192999999999999, 'precision': 0.7824698666306154, 'recall': 0.8848, 'f1': 0.8303853250923815, 'auc': 0.85451, 'fpr': 0.24620000000000003, 'tnr': 0.7538}\n",
      "Validate_prediction_by_links: {'accuracy': 0.7868181818181819, 'precision': 0.08359133126934984, 'recall': 0.135, 'f1': 0.10325047801147226, 'auc': 0.49845000000000006, 'fpr': 0.148}\n"
     ]
    }
   ],
   "source": [
    "twitter_config._name = \"twitter_\" + \"RandomForest\"\n",
    "learner = SkLearner(labels=labels)\n",
    "glc = GraphLearningController(learner, twitter_config)\n",
    "result_path = os.path.join(output_folder, twitter_config.name  + \"res.csv\")\n",
    "glc.classify_by_links(twitter_graph, \n",
    "                      result_path, \n",
    "                      test_size={\"neg\": 2000, \"pos\": 200},\n",
    "                      train_size={\"neg\": 5000, \"pos\": 5000}, \n",
    "                      meta_data_cols=meta_data_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting training and test sets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2299/10000 [00:00<00:00, 11362.73feature/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for training set:\n",
      "Graph loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 13518.10feature/s]\n",
      " 43%|████▎     | 946/2200 [00:00<00:00, 9457.75feature/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features were written to: /Users/vhying/Desktop/CS 221/Project/Network-Anomaly-Detection/data/temp/twitter_LogisticRegression__train.csv\n",
      "Extracting features for test set:\n",
      "Graph loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "70690feature [00:05, 13475.32feature/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features were written to: /Users/vhying/Desktop/CS 221/Project/Network-Anomaly-Detection/data/temp/twitter_LogisticRegression__test.csv\n",
      "Training 10-fold validation: {'accuracy': 0.829, 'precision': 0.7923047701868932, 'recall': 0.892, 'f1': 0.8391242529007122, 'auc': 0.8597158, 'fpr': 0.23399999999999999, 'tnr': 0.766}\n"
     ]
    }
   ],
   "source": [
    "twitter_config._name = \"twitter_\" + \"LogisticRegression\"\n",
    "IsoForest_learner = SkLearner(labels=labels).set_logistic_regression_classifier()\n",
    "glc = GraphLearningController(IsoForest_learner, twitter_config)\n",
    "result_path = os.path.join(output_folder, twitter_config.name  + \"res.csv\")\n",
    "glc.evaluate_classifier(twitter_graph, \n",
    "                        test_size={\"neg\": 2000, \"pos\": 200},\n",
    "                        training_size={\"neg\": 5000, \"pos\": 5000}, \n",
    "                        meta_data_cols=meta_data_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting training and test sets\n",
      "Existing files were loaded.\n",
      "Training 10-fold validation: {'accuracy': 0.8255000000000001, 'precision': 0.7894622375429599, 'recall': 0.8884000000000001, 'f1': 0.8358838719938493, 'auc': 0.8602497999999998, 'fpr': 0.2374, 'tnr': 0.7626000000000001}\n",
      "Validate_prediction_by_links: {'accuracy': 0.7836363636363637, 'precision': 0.08682634730538923, 'recall': 0.145, 'f1': 0.10861423220973783, 'auc': 0.46581249999999996, 'fpr': 0.1525}\n"
     ]
    }
   ],
   "source": [
    "twitter_config._name = \"twitter_\" + \"Adaboost\"\n",
    "learner = SkLearner(labels=labels).set_adaboost_classifier()\n",
    "glc = GraphLearningController(learner, twitter_config)\n",
    "result_path = os.path.join(output_folder, twitter_config.name  + \"res.csv\")\n",
    "glc.classify_by_links(twitter_graph, \n",
    "                      result_path,\n",
    "                      test_size={\"neg\": 2000, \"pos\": 200},\n",
    "                      train_size={\"neg\": 5000, \"pos\": 5000},\n",
    "                      meta_data_cols=meta_data_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting training and test sets\n",
      "Existing files were loaded.\n"
     ]
    }
   ],
   "source": [
    "twitter_config._name = \"twitter_\" + \"RFBagging\"\n",
    "learner = SkLearner(labels=labels).set_rf_bagging_classifier()\n",
    "glc = GraphLearningController(learner, twitter_config)\n",
    "result_path = os.path.join(output_folder, twitter_config.name  + \"res.csv\")\n",
    "glc.classify_by_links(twitter_graph, \n",
    "                      result_path,\n",
    "                      test_size={\"neg\": 2000, \"pos\": 200},\n",
    "                      train_size={\"neg\": 5000, \"pos\": 5000},\n",
    "                      meta_data_cols=meta_data_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twitter_config._name = \"twitter_\" + \"GradientBoosting\"\n",
    "learner = SkLearner(labels=labels).set_gradient_boosting_classifier()\n",
    "glc = GraphLearningController(learner, twitter_config)\n",
    "result_path = os.path.join(output_folder, twitter_config.name  + \"res.csv\")\n",
    "glc.classify_by_links(twitter_graph, \n",
    "                      result_path,\n",
    "                      test_size={\"neg\": 2000, \"pos\": 200},\n",
    "                      train_size={\"neg\": 5000, \"pos\": 5000},\n",
    "                      meta_data_cols=meta_data_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "twitter_config._name = \"twitter_\" + \"IsolationForest\"\n",
    "learner = SkLearner(labels=labels).set_isolation_forest_classifier()\n",
    "glc = GraphLearningController(learner, twitter_config)\n",
    "result_path = os.path.join(output_folder, twitter_config.name  + \"res.csv\")\n",
    "glc.classify_by_links(twitter_graph, \n",
    "                      result_path,\n",
    "                      test_size={\"neg\": 2000, \"pos\": 200},\n",
    "                      train_size={\"neg\": 5000, \"pos\": 5000},\n",
    "                      meta_data_cols=meta_data_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision at K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggreagate_res(data_folder, res_path):\n",
    "    results_frame = pd.DataFrame()\n",
    "    for f in os.listdir(data_folder):\n",
    "        temp_df = pd.read_csv(data_folder + \"/\" + f,index_col=0, encoding='utf-8', engine='python')\n",
    "        results_frame = results_frame.append(temp_df)\n",
    "    results_frame = results_frame.groupby(\"src_id\").mean()\n",
    "\n",
    "    return results_frame.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = aggreagate_res(output_folder, \"res.csv\").sort_values(\"mean_link_label\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to calculate precision at k, we add two additional columns:\n",
    "1. The sum of the positive examples.\n",
    "2. k  the total number of resutls until the specific row. (row number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"actual_sum\"] = df[\"actual\"].cumsum()\n",
    "df[\"k\"] = 1\n",
    "df[\"k\"] = df[\"k\"].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add an additional column that stores the p@k values by calculating the precision at k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"p@k\"] = df.apply(lambda x: x[\"actual_sum\"]/x[\"k\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[[\"k\", \"p@k\"]].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see in the p@k plot that the resuts are musch better then random which about 6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "df[[\"k\", \"p@k\"]][:500].plot(x=\"k\", y= \"p@k\")\n",
    "plt.plot(df[[\"k\"]].values, np.full((len(df[[\"k\"]]),1), 0.06))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
